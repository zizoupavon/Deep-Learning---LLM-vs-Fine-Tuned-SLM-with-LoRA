{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7879155,"sourceType":"datasetVersion","datasetId":4624298},{"sourceId":13759104,"sourceType":"datasetVersion","datasetId":8755678}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **MMAI 894 — Applied AI Project**\n## **Intent Classification with Large Language Model**\n### **Model: OpenAI GPT-4.1-mini**\n**Team: Museum**","metadata":{"_uuid":"82f9d458-0a60-4315-99e5-18b9bd4c9989","_cell_guid":"d30936d9-7e58-447d-b37f-4409a24757c9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ============================================================\n# FIX VERSION CONFLICTS — PIN COMPATIBLE VERSIONS\n# ============================================================\n!pip install -q protobuf==3.20.*\n!pip install -q --no-deps transformers==4.41.2 tokenizers==0.19.1 accelerate==0.33.0\n!pip install -q --no-deps peft==0.11.1 datasets==2.21.0\n!pip install -q scikit-learn==1.5.1\n!pip install -q ftfy faker\n!pip install -q langdetect sentencepiece\n!pip install -q openai","metadata":{"_uuid":"10250f56-a8b9-4570-af01-b1a6b26a37c6","_cell_guid":"3eb5f7e1-06f4-4c4b-8fba-378cfb6529a5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-22T08:39:15.139812Z","iopub.execute_input":"2025-11-22T08:39:15.140106Z","iopub.status.idle":"2025-11-22T08:40:07.129358Z","shell.execute_reply.started":"2025-11-22T08:39:15.140076Z","shell.execute_reply":"2025-11-22T08:40:07.128023Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ============================================================\n# IMPORTS\n# ============================================================\nimport os\nimport re\nimport random\nimport time\nimport numpy as np\nimport pandas as pd\nimport ftfy            \nfrom faker import Faker\n\n# ===========================\n# Machine Learning Tools\n# ===========================\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# ===========================\n# Language Detection\n# ===========================\nfrom langdetect import detect, DetectorFactory\nDetectorFactory.seed = 42\n\n# ===========================\n# Transformers / HuggingFace\n# ===========================\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n)\n\n# ===========================\n# OpenAI API\n# ===========================\nfrom kaggle_secrets import UserSecretsClient\nfrom openai import OpenAI","metadata":{"_uuid":"be24b34f-d46a-4557-b74b-08530326d88b","_cell_guid":"72869288-2366-4916-87cc-2bdbf7658d29","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-22T08:40:07.131729Z","iopub.execute_input":"2025-11-22T08:40:07.132296Z","iopub.status.idle":"2025-11-22T08:40:15.749240Z","shell.execute_reply.started":"2025-11-22T08:40:07.132254Z","shell.execute_reply":"2025-11-22T08:40:15.748319Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# **SECTION 1 — Data Preparation Pipeline**\n#### *Cleaning • Placeholder Augmentation • Category Tagging • Train/Val/Test Split*\n---","metadata":{"_uuid":"f048204b-4cff-4f50-a24c-d6a5cf8fab30","_cell_guid":"8a245eea-fa13-42e2-823c-56b3e2c34d0f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ============================================================\n# STEP 1 — LOAD DATASET\n# ============================================================\nROOT = \"../kaggle/\" if \"KAGGLE_KERNEL_RUN_TYPE\" not in os.environ else \"/kaggle/\"\n\nINPUT_DIR = ROOT + \"input/bitext-gen-ai-chatbot-customer-support-dataset\"\nOUTPUT_DIR = ROOT + \"working\"\nFILE_NAME = \"Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv\"\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\ndf = pd.read_csv(os.path.join(INPUT_DIR, FILE_NAME))\ndf = df[[\"instruction\", \"category\", \"intent\"]].copy()\n\nprint(\"Loaded:\", df.shape)\n\n\n# ============================================================\n# STEP 2 — FIX MOJIBAKE (â€™ → ’ etc.)\n# ============================================================\ndef fix_mojibake(text):\n    try:\n        return ftfy.fix_text(text)\n    except:\n        return text\n\ndf[\"instruction\"] = df[\"instruction\"].astype(str).apply(fix_mojibake)\n\n\n# ============================================================\n# STEP 3 — PLACEHOLDER REPLACEMENT\n# ============================================================\n\nPLACEHOLDER_RE = re.compile(r\"\\{\\{(.*?)\\}\\}\")\n\ndef generate_placeholder_value(placeholder: str, faker: Faker) -> str:\n    \"\"\"\n    Generate a value for a given placeholder name using Faker.\n    \"\"\"\n    ph = placeholder.strip()\n\n    if ph == \"Order Number\":\n        return f\"ORD{faker.random_int(min=100000, max=999999)}\"\n\n    elif ph == \"Account Type\":\n        return faker.random_element(elements=[\n            \"Checking Account\", \"Savings Account\",\n            \"Credit Card\", \"Business Account\"\n        ])\n\n    elif ph == \"Person Name\":\n        return faker.name()\n\n    elif ph == \"Account Category\":\n        return faker.random_element(elements=[\n            \"Personal\", \"Business\", \"Premium\", \"Student\"\n        ])\n\n    elif ph == \"Currency Symbol\":\n        return faker.random_element(elements=[\"$\", \"€\", \"£\"])\n\n    elif ph == \"Refund Amount\":\n        amount = faker.pyfloat(min_value=5, max_value=250, right_digits=2)\n        return f\"{amount:.2f}\"\n\n    elif ph == \"Delivery City\":\n        return faker.city()\n\n    elif ph == \"Delivery Country\":\n        return faker.country()\n\n    elif ph == \"Invoice Number\":\n        return f\"INV{faker.random_int(min=10000, max=99999)}\"\n\n    return ph  # fallback\n\n\ndef fill_instruction_placeholders(\n    df: pd.DataFrame,\n    col: str = \"instruction\",\n    base_seed: int = 42,\n    locale: str = \"en_US\",\n) -> pd.DataFrame:\n    \"\"\"\n    Replace {{placeholder}} with concrete values using Faker.\n    \"\"\"\n    out = df.copy()\n\n    def process_row(idx, text: str) -> str:\n        text = str(text)\n        placeholders = PLACEHOLDER_RE.findall(text)\n        if not placeholders:\n            return text\n\n        faker = Faker(locale)\n        faker.seed_instance(base_seed + int(idx))\n\n        row_map = {}\n        for ph in placeholders:\n            key = ph.strip()\n            if key not in row_map:\n                row_map[key] = generate_placeholder_value(key, faker)\n\n        def repl(match):\n            ph_raw = match.group(1).strip()\n            return row_map.get(ph_raw, ph_raw)\n\n        return PLACEHOLDER_RE.sub(repl, text)\n\n    out[col] = [process_row(idx, val) for idx, val in out[col].items()]\n    return out\n\n\ndf = fill_instruction_placeholders(\n    df,\n    col=\"instruction\",\n    base_seed=123,\n    locale=\"en_US\",\n)\n\n\n# ============================================================\n# STEP 4A — CLEAN MINOR GARBAGE (KEEP NATURAL ERRORS)\n# ============================================================\ndef clean_garbage(text):\n    text = re.sub(r\"[�]+\", \"\", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\ndf[\"instruction\"] = df[\"instruction\"].apply(clean_garbage)\n\n\n# ============================================================\n# STEP 4B — ADD STRUCTURED CATEGORY TAG FOR FT + LLM\n# ============================================================\ndf[\"instruction\"] = df.apply(\n    lambda row: f\"[CATEGORY={row['category']}] \" + row[\"instruction\"],\n    axis=1\n)\n\n\n# ============================================================\n# STEP 4C — REMOVE ORIGINAL CATEGORY COLUMN\n# ============================================================\ndf = df.drop(columns=[\"category\"])\n\n\n# ============================================================\n# STEP 5 — SAMPLE 15 ROWS TO VERIFY OUTPUT\n# ============================================================\nprint(\"\\n=== Sample cleaned rows ===\")\nprint(df.sample(15, random_state=SEED))\n\n\n# ============================================================\n# STEP 6 — SAVE CLEANED DATASET\n# ============================================================\ncleaned_path = os.path.join(OUTPUT_DIR, \"cleaned_bitext.csv\")\ndf.to_csv(cleaned_path, index=False)\nprint(\"\\nSaved cleaned dataset →\", cleaned_path)\n\n\n# ============================================================\n# STEP 7 — TRAIN/VAL/TEST SPLIT (80/10/10)\n# ============================================================\ntrain_df, temp_df = train_test_split(\n    df, test_size=0.2, random_state=SEED, stratify=df[\"intent\"]\n)\n\nval_df, test_df = train_test_split(\n    temp_df, test_size=0.5, random_state=SEED, stratify=temp_df[\"intent\"]\n)\n\ntrain_df.to_csv(os.path.join(OUTPUT_DIR, \"train.csv\"), index=False)\nval_df.to_csv(os.path.join(OUTPUT_DIR, \"val.csv\"), index=False)\ntest_df.to_csv(os.path.join(OUTPUT_DIR, \"test.csv\"), index=False)\n\nprint(\"\\nSaved train/val/test splits.\")\nprint(train_df.shape, val_df.shape, test_df.shape)","metadata":{"_uuid":"1e2de2c4-948e-43a4-979a-d1ca23c2a0a8","_cell_guid":"139abf37-2136-4f91-a339-b5df47f53100","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-22T08:40:15.750274Z","iopub.execute_input":"2025-11-22T08:40:15.750749Z","iopub.status.idle":"2025-11-22T08:40:25.596374Z","shell.execute_reply.started":"2025-11-22T08:40:15.750720Z","shell.execute_reply":"2025-11-22T08:40:25.595438Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Loaded: (26872, 3)\n\n=== Sample cleaned rows ===\n                                             instruction  \\\n9329   [CATEGORY=CONTACT] I can't talk with a human a...   \n4160   [CATEGORY=INVOICE] I have got to locate hte bi...   \n18500  [CATEGORY=PAYMENT] I cannot pay, help me to in...   \n8840   [CATEGORY=CONTACT] I want help speaking to cus...   \n5098   [CATEGORY=PAYMENT] I try to see th accepted pa...   \n17250  [CATEGORY=SUBSCRIPTION] where to sign up to th...   \n3589   [CATEGORY=CANCEL] I'd like to see the withdrwa...   \n9043     [CATEGORY=CONTACT] I want to speak with someone   \n15800  [CATEGORY=INVOICE] can you help me getting bil...   \n4384   [CATEGORY=INVOICE] I don't know how to take a ...   \n11150  [CATEGORY=ACCOUNT] I don't know how to delete ...   \n6417   [CATEGORY=REFUND] help me check in what cases ...   \n4186   [CATEGORY=INVOICE] is it possible to locate my...   \n7301   [CATEGORY=FEEDBACK] i want help to file a cons...   \n8267   [CATEGORY=CONTACT] uhave a free number to call...   \n\n                         intent  \n9329        contact_human_agent  \n4160              check_invoice  \n18500             payment_issue  \n8840   contact_customer_service  \n5098      check_payment_methods  \n17250   newsletter_subscription  \n3589     check_cancellation_fee  \n9043        contact_human_agent  \n15800               get_invoice  \n4384              check_invoice  \n11150            delete_account  \n6417        check_refund_policy  \n4186              check_invoice  \n7301                  complaint  \n8267   contact_customer_service  \n\nSaved cleaned dataset → /kaggle/working/cleaned_bitext.csv\n\nSaved train/val/test splits.\n(21497, 2) (2687, 2) (2688, 2)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# **SECTION 2 — GPT-4.1-mini LLM Evaluation Pipeline**\n#### *Sampling • Prompt Construction • Inference • Accuracy • Macro-F1 • Cost & Time Estimation*\n---","metadata":{"_uuid":"9be2dcd8-b7d2-4f45-9982-c80a155485da","_cell_guid":"09842209-1e24-4789-b96c-82bedba54131","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ============================================================\n# STEP 0 Load API Keys\n# ============================================================\n\n# Load API key\nuser_secrets = UserSecretsClient()\nOPENAI_API_KEY = user_secrets.get_secret(\"OPENAI_API_KEY\")\nclient = OpenAI(api_key=OPENAI_API_KEY)\n\n\n# ============================================================\n# STEP 1 — Load validation set & sample 270 items\n# (10 samples per intent as required by assignment)\n# ============================================================\n\nval_df = pd.read_csv(\"/kaggle/working/val.csv\")\n\nsample_df = (\n    val_df.groupby(\"intent\")\n          .head(10)\n          .reset_index(drop=True)\n)\n\nprint(\"Sample size:\", sample_df.shape)\nsample_df.head()\n\n\n# STEP 2 — Prepare intent list for LLM prompt\nall_intents = sorted(val_df[\"intent\"].unique())\nintent_list_str = \", \".join(all_intents)\nprint(\"Loaded intents:\", len(all_intents))\n\n# STEP 3 — Build bullet-style intent list (required!)\navailable_intents_bulleted = \"\\n- \" + \"\\n- \".join(all_intents)\n\n\n\n# ============================================================\n# STEP 3B — Define GPT-4.1-mini classifier\n# ============================================================\ndef classify_gpt41mini(instruction):\n    prompt = f\"\"\"\nYou are an intent classification assistant for customer support messages.\nYour task is to identify the single best intent that matches what the user is trying to accomplish.\nYou MUST output exactly one intent from the allowed list.\n\nUser messages often begin with:\n[CATEGORY=XXX] <instruction>\nCATEGORY indicates which group of intents the message belongs to. \nYou MUST pick an intent ONLY from the intents listed under that CATEGORY. Never choose an intent from any other category.\n\nHere are the valid categories and their allowed intents:\n\nACCOUNT:\n- create_account\n- delete_account\n- edit_account\n- recover_password\n- registration_problems\n- switch_account\n\nCANCELLATION_FEE:\n- check_cancellation_fee\n\nCONTACT:\n- contact_customer_service\n- contact_human_agent\n\nDELIVERY:\n- delivery_options\n- delivery_period\n\nFEEDBACK:\n- complaint\n- review\n\nINVOICE:\n- check_invoice\n- get_invoice\n\nORDER:\n- cancel_order\n- change_order\n- place_order\n- track_order\n\nPAYMENT:\n- check_payment_methods\n- payment_issue\n\nREFUND:\n- check_refund_policy\n- get_refund\n- track_refund\n\nSHIPPING_ADDRESS:\n- change_shipping_address\n- set_up_shipping_address\n\nSUBSCRIPTION:\n- newsletter_subscription\n\nSoft guidelines (not strict rules):\n- Refund messages may involve requesting a refund, checking status, or asking about refund rules.\n- Delivery questions may be about timing, availability, or tracking.\n- Setting up a new address ≠ editing an existing address.\n- Account problems may involve recovery, switching profiles, or editing information.\n- Asking for help does not automatically mean contact_human_agent.\n- Invoice questions may involve locating an existing invoice or requesting a copy.\n\nAfter determining the best intent, output ONLY the intent label in this exact format:\nFinal: <intent>\n\nDo NOT output reasoning, sentences, bullet points, or any other text.\n\nUser message:\n\"{instruction}\"\n\n\"\"\"\n\n\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4.1-mini\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=0,\n            max_tokens=64\n        )\n        output = response.choices[0].message.content.strip()\n\n        # === Extract the final intent ===\n        match = re.search(r\"Final:\\s*([a-zA-Z0-9_]+)\", output)\n        if match:\n            return match.group(1).strip()\n        else:\n            # fallback: try last line\n            last_line = output.splitlines()[-1].strip()\n            return last_line.replace(\"Final:\", \"\").strip()\n\n    except Exception as e:\n        print(\"Error:\", e)\n        return \"ERROR\"\n\n\n# ============================================================\n# STEP 4 — Run inference on all 270 samples\n# ============================================================\n\npreds = []\nstart = time.time()\n\nfor i, row in sample_df.iterrows():\n    msg = row[\"instruction\"]\n    pred = classify_gpt41mini(msg)\n    preds.append(pred)\n\n    if i % 20 == 0:\n        print(f\"Processed {i}/{len(sample_df)}\")\n\nend = time.time()\nelapsed = end - start\n\n\nsample_df[\"gpt41mini_pred\"] = preds\n\nsave_path = \"/kaggle/working/gpt41mini_eval.csv\"\nsample_df.to_csv(save_path, index=False)\nprint(\"Predictions saved →\", save_path)\n\n\n# ============================================================\n# STEP 5 — Compute Accuracy & Macro-F1\n# ============================================================\n\ny_true = sample_df[\"intent\"]\ny_pred = sample_df[\"gpt41mini_pred\"]\n\nacc = accuracy_score(y_true, y_pred)\nmacro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n\nprint(\"\\n===== GPT-4.1-mini Evaluation =====\")\nprint(\"Accuracy:\", acc)\nprint(\"Macro-F1:\", macro_f1)\n\n\n# ============================================================\n# STEP 6 — Estimate inference time per 1000 samples\n# ============================================================\n\ntime_per_sample = elapsed / len(sample_df)\ntime_per_1000 = time_per_sample * 1000\n\nprint(\"\\nInference time for 270 samples:\", elapsed, \"seconds\")\nprint(\"Estimated inference time per 1000 samples:\", time_per_1000, \"seconds\")\n\n\n# ============================================================\n# STEP 7 — Estimate API cost (per assignment requirement)\n# ============================================================\n\n# GPT-4.1-mini pricing:\n# Input: $0.06 per 1M tokens\n# Output: $0.12 per 1M tokens\n\navg_input_tokens = 350   # prompt + instruction\navg_output_tokens = 3    # label only\n\ncost_per_1000 = (\n    (avg_input_tokens * 1000 / 1_000_000) * 0.06 +\n    (avg_output_tokens * 1000 / 1_000_000) * 0.12\n)\n\nprint(\"\\nEstimated API cost per 1000 samples: $\", round(cost_per_1000, 5))\nprint(\"\\n===== DONE =====\")","metadata":{"_uuid":"1048c782-be01-4957-92dc-3c48556757ef","_cell_guid":"17ae32af-164c-4074-b84a-d99381424878","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-22T08:40:25.597300Z","iopub.execute_input":"2025-11-22T08:40:25.597657Z","iopub.status.idle":"2025-11-22T08:43:19.737910Z","shell.execute_reply.started":"2025-11-22T08:40:25.597626Z","shell.execute_reply":"2025-11-22T08:43:19.736979Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Sample size: (270, 2)\nLoaded intents: 27\nProcessed 0/270\nProcessed 20/270\nProcessed 40/270\nProcessed 60/270\nProcessed 80/270\nProcessed 100/270\nProcessed 120/270\nProcessed 140/270\nProcessed 160/270\nProcessed 180/270\nProcessed 200/270\nProcessed 220/270\nProcessed 240/270\nProcessed 260/270\nPredictions saved → /kaggle/working/gpt41mini_eval.csv\n\n===== GPT-4.1-mini Evaluation =====\nAccuracy: 0.9666666666666667\nMacro-F1: 0.9663580950130658\n\nInference time for 270 samples: 173.5626242160797 seconds\nEstimated inference time per 1000 samples: 642.8245341336286 seconds\n\nEstimated API cost per 1000 samples: $ 0.02136\n\n===== DONE =====\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# **SECTION 3 — GPT-4.1-mini Test Set Inference & Submission Pipeline**\n#### *Cleaning • Placeholder Augmentation • Category Tagging • LLM Inference • Submission Build • Cost & Time Estimation*\n---\n`","metadata":{"_uuid":"cd3ed6c5-9ace-4a56-8edb-cd75480437de","_cell_guid":"b2563ead-80b2-470c-af77-925471317379","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ============================================================\n# STEP A — Load raw test.csv\n# ============================================================\n\nTEST_PATH = \"/kaggle/input/test-csv/test.csv\"\ntest_raw = pd.read_csv(TEST_PATH)\n\nprint(\"Loaded raw test:\", test_raw.shape)\ntest_raw.head()\n\n\n# ============================================================\n# STEP B — Apply EXACT SAME CLEANING PIPELINE as TRAIN\n# ============================================================\n\n# --- 1) Fix mojibake ---\ntest_raw[\"instruction\"] = (\n    test_raw[\"instruction\"]\n        .astype(str)\n        .apply(fix_mojibake)\n)\n\n# --- 2) Replace placeholders (USE THE SAME FUNCTION AS TRAIN) ---\ntest_raw = fill_instruction_placeholders(\n    test_raw,\n    col=\"instruction\",\n    base_seed=123,\n    locale=\"en_US\"\n)\n\n# --- 3) Clean garbage ---\ntest_raw[\"instruction\"] = test_raw[\"instruction\"].apply(clean_garbage)\n\n# --- 4) Add category tag ---\nif \"category\" in test_raw.columns:\n    test_raw[\"instruction\"] = test_raw.apply(\n        lambda row: f\"[CATEGORY={row['category']}] \" + row[\"instruction\"],\n        axis=1\n    )\nelse:\n    def add_tag_auto(t):\n        m = re.search(r\"\\[CATEGORY=([A-Z_]+)\\]\", t)\n        if m:\n            return t\n        return \"[CATEGORY=UNKNOWN] \" + t\n    test_raw[\"instruction\"] = test_raw[\"instruction\"].apply(add_tag_auto)\n\n\n\n# ============================================================\n# STEP C — Run inference on test rows\n# ============================================================\n\ntest_preds = []\nstart = time.time()\n\nfor i, row in test_raw.iterrows():\n    msg = row[\"instruction\"]\n    pred = classify_gpt41mini(msg)\n    test_preds.append(pred)\n\n    if i % 20 == 0:\n        print(f\"Processed {i}/{len(test_raw)}\")\n\nend = time.time()\nelapsed_test = end - start\n\n\n# ============================================================\n# STEP D — Build submission file\n# ============================================================\n\nsubmission = pd.DataFrame({\n    \"id\": test_raw[\"id\"],\n    \"intent\": test_preds\n})\n\nsub_path = \"/kaggle/working/submission.csv\"\nsubmission.to_csv(sub_path, index=False)\n\nprint(\"\\nSaved submission →\", sub_path)\nprint(submission.head())\n\n\n# ============================================================\n# STEP E — Report inference speed + cost\n# ============================================================\n\ntime_per_sample_test = elapsed_test / len(test_raw)\ntime_per_1000_test = time_per_sample_test * 1000\n\nprint(\"\\n===== INFERENCE REPORT (TEST SET) =====\")\nprint(\"Total inference time:\", round(elapsed_test, 2), \"seconds\")\nprint(\"Estimated time per 1000 samples:\", round(time_per_1000_test, 2), \"seconds\")\n\n# --- Same cost formula as validation ---\navg_input_tokens = 350\navg_output_tokens = 3\n\nestimated_cost_per_1000 = (\n    (avg_input_tokens * 1000 / 1_000_000) * 0.06 +\n    (avg_output_tokens * 1000 / 1_000_000) * 0.12\n)\n\nprint(\"Estimated API cost per 1000 samples: $\", round(estimated_cost_per_1000, 5))\nprint(\"===== DONE =====\")","metadata":{"_uuid":"b2eba428-4dac-4499-9c2a-f5494259c564","_cell_guid":"49582623-35dc-4be9-83b5-e541e2103699","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-22T08:43:19.738984Z","iopub.execute_input":"2025-11-22T08:43:19.739242Z","iopub.status.idle":"2025-11-22T08:45:57.767917Z","shell.execute_reply.started":"2025-11-22T08:43:19.739220Z","shell.execute_reply":"2025-11-22T08:45:57.766940Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Loaded raw test: (270, 5)\nProcessed 0/270\nProcessed 20/270\nProcessed 40/270\nProcessed 60/270\nProcessed 80/270\nProcessed 100/270\nProcessed 120/270\nProcessed 140/270\nProcessed 160/270\nProcessed 180/270\nProcessed 200/270\nProcessed 220/270\nProcessed 240/270\nProcessed 260/270\n\nSaved submission → /kaggle/working/submission.csv\n   id                    intent\n0   1  contact_customer_service\n1   2            switch_account\n2   3  contact_customer_service\n3   4            create_account\n4   5       contact_human_agent\n\n===== INFERENCE REPORT (TEST SET) =====\nTotal inference time: 157.96 seconds\nEstimated time per 1000 samples: 585.05 seconds\nEstimated API cost per 1000 samples: $ 0.02136\n===== DONE =====\n","output_type":"stream"}],"execution_count":5}]}