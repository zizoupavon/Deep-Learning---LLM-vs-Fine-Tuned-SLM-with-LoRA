{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7879155,"sourceType":"datasetVersion","datasetId":4624298},{"sourceId":13759104,"sourceType":"datasetVersion","datasetId":8755678}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **MMAI 894 — Applied AI Project**\n## **Intent Classification with Fine-Tuned Transformer**\n### **Model: DistilRoBERTa + LoRA**\n**Team: Museum**","metadata":{"_uuid":"965d317f-4749-40f4-b175-6492b56fd8f0","_cell_guid":"53dd9096-c8bb-4b2e-ba4c-7cfc70e778c9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ============================================================\n# FIX VERSION CONFLICTS — PIN COMPATIBLE VERSIONS\n# ============================================================\n!pip install -q protobuf==3.20.*\n!pip install -q --no-deps transformers==4.41.2 tokenizers==0.19.1 accelerate==0.33.0\n!pip install -q --no-deps peft==0.11.1 datasets==2.21.0\n!pip install -q scikit-learn==1.5.1\n!pip install -q ftfy faker","metadata":{"_uuid":"7a99aab0-2964-4dc7-82f4-da60b6252d62","_cell_guid":"5f2f9173-c464-45ec-9bcf-4a83fbb2d214","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-22T18:52:23.998207Z","iopub.execute_input":"2025-11-22T18:52:23.998462Z","iopub.status.idle":"2025-11-22T18:52:56.058839Z","shell.execute_reply.started":"2025-11-22T18:52:23.998438Z","shell.execute_reply":"2025-11-22T18:52:56.058059Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m997.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ============================================================\n# IMPORTS\n# ============================================================\nimport os, re, random, time\nimport numpy as np\nimport pandas as pd\nimport ftfy\nfrom faker import Faker\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Torch & HF Datasets\nimport torch\nfrom datasets import Dataset\n\n# HuggingFace Transformers\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    DataCollatorWithPadding,\n    TrainingArguments,\n    Trainer,\n    EarlyStoppingCallback,\n    MarianMTModel,\n    MarianTokenizer,\n)\n\n# PEFT (LoRA)\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    TaskType,\n    PeftModel,\n    PeftConfig\n)","metadata":{"_uuid":"d6af6433-e06b-48fb-8045-45bc9c293294","_cell_guid":"0b486dab-383b-4adb-a918-8c97b8790863","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-22T18:52:56.060910Z","iopub.execute_input":"2025-11-22T18:52:56.061171Z","iopub.status.idle":"2025-11-22T18:53:17.737708Z","shell.execute_reply.started":"2025-11-22T18:52:56.061145Z","shell.execute_reply":"2025-11-22T18:53:17.737124Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","text":"2025-11-22 18:53:03.784957: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763837583.988269      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763837584.044697      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# **SECTION 1 — Data Preparation Pipeline**\n#### *Cleaning • Placeholder Augmentation • Category Tagging • Train/Val/Test Split*\n---","metadata":{"_uuid":"c1f2d8d6-cf55-465f-b0c0-454bc0ce31e8","_cell_guid":"4bc3477f-d0ca-4ffd-bcb5-ae56337e6d2a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ============================================================\n# STEP 1 — LOAD DATASET\n# ============================================================\nROOT = \"../kaggle/\" if \"KAGGLE_KERNEL_RUN_TYPE\" not in os.environ else \"/kaggle/\"\n\nINPUT_DIR = ROOT + \"input/bitext-gen-ai-chatbot-customer-support-dataset\"\nOUTPUT_DIR = ROOT + \"working\"\nFILE_NAME = \"Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv\"\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\ndf = pd.read_csv(os.path.join(INPUT_DIR, FILE_NAME))\ndf = df[[\"instruction\", \"category\", \"intent\"]].copy()\n\nprint(\"Loaded:\", df.shape)\n\n\n# ============================================================\n# STEP 2 — FIX MOJIBAKE (â€™ → ’ etc.)\n# ============================================================\ndef fix_mojibake(text):\n    try:\n        return ftfy.fix_text(text)\n    except:\n        return text\n\ndf[\"instruction\"] = df[\"instruction\"].astype(str).apply(fix_mojibake)\n\n\n# ============================================================\n# STEP 3 — PLACEHOLDER REPLACEMENT\n# ============================================================\n\nPLACEHOLDER_RE = re.compile(r\"\\{\\{(.*?)\\}\\}\")\n\ndef generate_placeholder_value(placeholder: str, faker: Faker) -> str:\n    \"\"\"\n    Generate a value for a given placeholder name using Faker.\n    \"\"\"\n    ph = placeholder.strip()\n\n    if ph == \"Order Number\":\n        return f\"ORD{faker.random_int(min=100000, max=999999)}\"\n\n    elif ph == \"Account Type\":\n        return faker.random_element(elements=[\n            \"Checking Account\", \"Savings Account\",\n            \"Credit Card\", \"Business Account\"\n        ])\n\n    elif ph == \"Person Name\":\n        return faker.name()\n\n    elif ph == \"Account Category\":\n        return faker.random_element(elements=[\n            \"Personal\", \"Business\", \"Premium\", \"Student\"\n        ])\n\n    elif ph == \"Currency Symbol\":\n        return faker.random_element(elements=[\"$\", \"€\", \"£\"])\n\n    elif ph == \"Refund Amount\":\n        amount = faker.pyfloat(min_value=5, max_value=250, right_digits=2)\n        return f\"{amount:.2f}\"\n\n    elif ph == \"Delivery City\":\n        return faker.city()\n\n    elif ph == \"Delivery Country\":\n        return faker.country()\n\n    elif ph == \"Invoice Number\":\n        return f\"INV{faker.random_int(min=10000, max=99999)}\"\n\n    return ph  # fallback\n\n\ndef fill_instruction_placeholders(\n    df: pd.DataFrame,\n    col: str = \"instruction\",\n    base_seed: int = 42,\n    locale: str = \"en_US\",\n) -> pd.DataFrame:\n    \"\"\"\n    Replace {{placeholder}} with concrete values using Faker.\n    \"\"\"\n    out = df.copy()\n\n    def process_row(idx, text: str) -> str:\n        text = str(text)\n        placeholders = PLACEHOLDER_RE.findall(text)\n        if not placeholders:\n            return text\n\n        faker = Faker(locale)\n        faker.seed_instance(base_seed + int(idx))\n\n        row_map = {}\n        for ph in placeholders:\n            key = ph.strip()\n            if key not in row_map:\n                row_map[key] = generate_placeholder_value(key, faker)\n\n        def repl(match):\n            ph_raw = match.group(1).strip()\n            return row_map.get(ph_raw, ph_raw)\n\n        return PLACEHOLDER_RE.sub(repl, text)\n\n    out[col] = [process_row(idx, val) for idx, val in out[col].items()]\n    return out\n\n\ndf = fill_instruction_placeholders(\n    df,\n    col=\"instruction\",\n    base_seed=123,\n    locale=\"en_US\",\n)\n\n\n# ============================================================\n# STEP 4A — CLEAN MINOR GARBAGE (KEEP NATURAL ERRORS)\n# ============================================================\ndef clean_garbage(text):\n    text = re.sub(r\"[�]+\", \"\", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\ndf[\"instruction\"] = df[\"instruction\"].apply(clean_garbage)\n\n\n# ============================================================\n# STEP 4B — ADD STRUCTURED CATEGORY TAG FOR FT + LLM\n# ============================================================\ndf[\"instruction\"] = df.apply(\n    lambda row: f\"[CATEGORY={row['category']}] \" + row[\"instruction\"],\n    axis=1\n)\n\n\n# ============================================================\n# STEP 4C — REMOVE ORIGINAL CATEGORY COLUMN\n# ============================================================\ndf = df.drop(columns=[\"category\"])\n\n\n# ============================================================\n# STEP 5 — SAMPLE 15 ROWS TO VERIFY OUTPUT\n# ============================================================\nprint(\"\\n=== Sample cleaned rows ===\")\nprint(df.sample(15, random_state=SEED))\n\n\n# ============================================================\n# STEP 6 — SAVE CLEANED DATASET\n# ============================================================\ncleaned_path = os.path.join(OUTPUT_DIR, \"cleaned_bitext.csv\")\ndf.to_csv(cleaned_path, index=False)\nprint(\"\\nSaved cleaned dataset →\", cleaned_path)\n\n\n# ============================================================\n# STEP 7 — TRAIN/VAL/TEST SPLIT (80/10/10)\n# ============================================================\ntrain_df, temp_df = train_test_split(\n    df, test_size=0.2, random_state=SEED, stratify=df[\"intent\"]\n)\n\nval_df, test_df = train_test_split(\n    temp_df, test_size=0.5, random_state=SEED, stratify=temp_df[\"intent\"]\n)\n\ntrain_df.to_csv(os.path.join(OUTPUT_DIR, \"train.csv\"), index=False)\nval_df.to_csv(os.path.join(OUTPUT_DIR, \"val.csv\"), index=False)\ntest_df.to_csv(os.path.join(OUTPUT_DIR, \"test.csv\"), index=False)\n\nprint(\"\\nSaved train/val/test splits.\")\nprint(train_df.shape, val_df.shape, test_df.shape)","metadata":{"_uuid":"c12bcf37-e328-4fa4-b2b8-8466dffce583","_cell_guid":"f8f0e9a1-4bcf-4c19-a705-d10c0101d55f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-22T18:53:17.738464Z","iopub.execute_input":"2025-11-22T18:53:17.739080Z","iopub.status.idle":"2025-11-22T18:53:25.978606Z","shell.execute_reply.started":"2025-11-22T18:53:17.739060Z","shell.execute_reply":"2025-11-22T18:53:25.977892Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Loaded: (26872, 3)\n\n=== Sample cleaned rows ===\n                                             instruction  \\\n9329   [CATEGORY=CONTACT] I can't talk with a human a...   \n4160   [CATEGORY=INVOICE] I have got to locate hte bi...   \n18500  [CATEGORY=PAYMENT] I cannot pay, help me to in...   \n8840   [CATEGORY=CONTACT] I want help speaking to cus...   \n5098   [CATEGORY=PAYMENT] I try to see th accepted pa...   \n17250  [CATEGORY=SUBSCRIPTION] where to sign up to th...   \n3589   [CATEGORY=CANCEL] I'd like to see the withdrwa...   \n9043     [CATEGORY=CONTACT] I want to speak with someone   \n15800  [CATEGORY=INVOICE] can you help me getting bil...   \n4384   [CATEGORY=INVOICE] I don't know how to take a ...   \n11150  [CATEGORY=ACCOUNT] I don't know how to delete ...   \n6417   [CATEGORY=REFUND] help me check in what cases ...   \n4186   [CATEGORY=INVOICE] is it possible to locate my...   \n7301   [CATEGORY=FEEDBACK] i want help to file a cons...   \n8267   [CATEGORY=CONTACT] uhave a free number to call...   \n\n                         intent  \n9329        contact_human_agent  \n4160              check_invoice  \n18500             payment_issue  \n8840   contact_customer_service  \n5098      check_payment_methods  \n17250   newsletter_subscription  \n3589     check_cancellation_fee  \n9043        contact_human_agent  \n15800               get_invoice  \n4384              check_invoice  \n11150            delete_account  \n6417        check_refund_policy  \n4186              check_invoice  \n7301                  complaint  \n8267   contact_customer_service  \n\nSaved cleaned dataset → /kaggle/working/cleaned_bitext.csv\n\nSaved train/val/test splits.\n(21497, 2) (2687, 2) (2688, 2)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# **SECTION 2 — Fine-Tune DistilRoBERTa with LoRA**\n#### *Dataset Loading • Label Encoding • Tokenization • LoRA Adapter • Training & Saving*\n---","metadata":{"_uuid":"21421ba7-2bc4-45f2-978d-7bcbb9b69cf8","_cell_guid":"42db2336-c3cd-41f6-bb41-b85e35754df2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ============================================================\n# STEP 1 — Load train/val splits\n# ============================================================\n\nprint(\"Train shape:\", train_df.shape)\nprint(\"Val shape:\", val_df.shape)\n\n# ============================================================\n# STEP 2 — Build label mapping\n# ============================================================\nlabels = sorted(train_df[\"intent\"].unique())\nlabel2id = {lbl: i for i, lbl in enumerate(labels)}\nid2label = {i: lbl for lbl, i in label2id.items()}\nnum_labels = len(labels)\nprint(\"Num labels:\", num_labels)\n\n\n# ============================================================\n# STEP 3 — Create HuggingFace Datasets\n# ============================================================\n\ntrain_df = train_df.reset_index(drop=True)\nval_df   = val_df.reset_index(drop=True)\n\ntrain_ds = Dataset.from_pandas(train_df[[\"instruction\", \"intent\"]])\nval_ds   = Dataset.from_pandas(val_df[[\"instruction\", \"intent\"]])\n\ntrain_ds = train_ds.rename_column(\"intent\", \"label\")\nval_ds   = val_ds.rename_column(\"intent\", \"label\")\n\ndef encode_labels(example):\n    return {\"label\": label2id[example[\"label\"]]}\n\ntrain_ds = train_ds.map(encode_labels)\nval_ds   = val_ds.map(encode_labels)\n\n# ============================================================\n# STEP 4 — Tokenize and Encode Samples\n# ============================================================\n\nBASE_MODEL_NAME = \"distilroberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n\ndef encode_batch(batch):\n    return tokenizer(\n        batch[\"instruction\"],\n        truncation=True,\n        padding=False,\n        max_length=128,\n    )\n\ntrain_ds = train_ds.map(encode_batch, batched=True)\nval_ds   = val_ds.map(encode_batch, batched=True)\n\ntrain_ds = train_ds.remove_columns([\"instruction\"])\nval_ds   = val_ds.remove_columns([\"instruction\"])\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n\n# ============================================================\n# STEP 5 — Load Base Model and Apply LoRA Adapter\n# ============================================================\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    BASE_MODEL_NAME,\n    num_labels=num_labels,\n    id2label=id2label,\n    label2id=label2id,\n)\n\nlora_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    r=32,\n    lora_alpha=64,\n    lora_dropout=0.1,\n    bias=\"none\",\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\n\n# ============================================================\n# STEP 6 — Configure TrainingArguments and Trainer\n# ============================================================\n\nFT_DIR = ROOT + \"working/distilroberta_lora_ft\"\n\ntraining_args = TrainingArguments(\n    output_dir=FT_DIR,\n    learning_rate=5e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    num_train_epochs=10,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_steps=50,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    report_to=\"none\",\n)\n\ndef compute_metrics(eval_pred):\n    logits, labels_np = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    acc = accuracy_score(labels_np, preds)\n    macro_f1 = f1_score(labels_np, preds, average=\"macro\")\n    return {\"accuracy\": acc, \"f1\": macro_f1}\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[\n        EarlyStoppingCallback(\n            early_stopping_patience=3,\n            early_stopping_threshold=0.0001,\n        )\n    ],\n)\n\n# ============================================================\n# STEP 6B — Measure Training Time and GPU Cost\n# ============================================================\n\nstart_train = time.time()\ntrainer.train()\nend_train = time.time()\n\ntrain_seconds = end_train - start_train\ntrain_hours   = train_seconds / 3600\n\nprint(f\"\\nTraining time: {train_seconds:.2f} seconds ({train_hours:.4f} hours)\")\n\n# === GPU cost estimation (Google Cloud T4: $0.35 / hour) ===\nT4_PRICE = 0.35\ngpu_cost = train_hours * T4_PRICE\nprint(f\"Estimated GPU training cost on T4: ${gpu_cost:.4f} USD\")\n\n\n# ------------------------------------------------------------\n# 7. Save fine-tuned LoRA model\n# ------------------------------------------------------------\ntrainer.save_model(FT_DIR)\ntokenizer.save_pretrained(FT_DIR)\nprint(\"Saved fine-tuned LoRA model to:\", FT_DIR)","metadata":{"_uuid":"94d04f3a-7ac5-4f3b-808c-d508a10b12c9","_cell_guid":"8d17d48e-1017-4ceb-9a23-a00207c7477c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-22T18:53:25.979397Z","iopub.execute_input":"2025-11-22T18:53:25.979663Z","iopub.status.idle":"2025-11-22T18:59:29.027225Z","shell.execute_reply.started":"2025-11-22T18:53:25.979644Z","shell.execute_reply":"2025-11-22T18:59:29.026500Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Train shape: (21497, 2)\nVal shape: (2687, 2)\nNum labels: 27\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21497 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bad74e6e4eb448a4a5dbc3c428a6070d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2687 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00e43fa958dc42009e32f69489cf2ae7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6fa54adf87f4b26b80825412dd64718"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39e69fec8a744e2eb5be4830196a1ab4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52c144200a4a4e6cbc082fae69ed37fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25ffc02a7f6d4e1e86b38d7d7f329752"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3cbd8120e2449598a10eb7010c5be60"}},"metadata":{}},{"name":"stderr","text":"Parameter 'function'=<function encode_batch at 0x7befe42ce340> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21497 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a516300cb3ab4d62aa5fbd32b3148232"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2687 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8060a813576948b09991b93fef9d9e18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ef78954fdcc4b22a63f3ea0f2dba408"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 1,201,179 || all params: 83,340,342 || trainable%: 1.4413\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13440' max='13440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13440/13440 05:49, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.043100</td>\n      <td>0.027238</td>\n      <td>0.994045</td>\n      <td>0.994066</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.017500</td>\n      <td>0.020400</td>\n      <td>0.994790</td>\n      <td>0.994793</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.006000</td>\n      <td>0.023218</td>\n      <td>0.994790</td>\n      <td>0.994797</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.011000</td>\n      <td>0.020807</td>\n      <td>0.994418</td>\n      <td>0.994417</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.009500</td>\n      <td>0.019020</td>\n      <td>0.995162</td>\n      <td>0.995171</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.002500</td>\n      <td>0.021093</td>\n      <td>0.995906</td>\n      <td>0.995916</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.003700</td>\n      <td>0.019726</td>\n      <td>0.995534</td>\n      <td>0.995540</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.009000</td>\n      <td>0.019065</td>\n      <td>0.996278</td>\n      <td>0.996281</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.000300</td>\n      <td>0.018572</td>\n      <td>0.995162</td>\n      <td>0.995172</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.006500</td>\n      <td>0.018843</td>\n      <td>0.996278</td>\n      <td>0.996286</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nTraining time: 353.06 seconds (0.0981 hours)\nEstimated GPU training cost on T4: $0.0343 USD\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Saved fine-tuned LoRA model to: /kaggle/working/distilroberta_lora_ft\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# **SECTION 2B — Full Fine-Tuning DistilRoBERTa (No LoRA)**\n#### *Dataset Loading • Full Parameter Training • Early Stopping • Save Checkpoint*\n---\n","metadata":{}},{"cell_type":"code","source":"print(\"\\n===== SECTION 2B — Full Fine-Tuning DistilRoBERTa =====\")\n\nBASE_MODEL_NAME = \"distilroberta-base\"\n\n# Reload tokenizer (safe)\nfullft_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n\n# Rebuild dataset for Full FT (avoid shared references)\ntrain_ds_ff = Dataset.from_pandas(train_df[[\"instruction\", \"intent\"]]).rename_column(\"intent\", \"label\")\nval_ds_ff   = Dataset.from_pandas(val_df[[\"instruction\", \"intent\"]]).rename_column(\"intent\", \"label\")\n\n# Encode labels\ntrain_ds_ff = train_ds_ff.map(encode_labels)\nval_ds_ff   = val_ds_ff.map(encode_labels)\n\n# Tokenization\ntrain_ds_ff = train_ds_ff.map(encode_batch, batched=True)\nval_ds_ff   = val_ds_ff.map(encode_batch, batched=True)\n\ntrain_ds_ff = train_ds_ff.remove_columns([\"instruction\"])\nval_ds_ff   = val_ds_ff.remove_columns([\"instruction\"])\n\ndata_collator_ff = DataCollatorWithPadding(tokenizer=fullft_tokenizer)\n\n# ------------------------------------------------------------\n# Load base DistilRoBERTa for FULL fine-tuning (ALL params trainable)\n# ------------------------------------------------------------\nfullft_model = AutoModelForSequenceClassification.from_pretrained(\n    BASE_MODEL_NAME,\n    num_labels=num_labels,\n    id2label=id2label,\n    label2id=label2id\n)\n\nFULLFT_DIR = ROOT + \"working/distilroberta_full_ft\"\n\ntraining_args_full = TrainingArguments(\n    output_dir=FULLFT_DIR,\n    learning_rate=3e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    num_train_epochs=8,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    logging_steps=50,\n    report_to=\"none\",\n)\n\ntrainer_full = Trainer(\n    model=fullft_model,\n    args=training_args_full,\n    train_dataset=train_ds_ff,\n    eval_dataset=val_ds_ff,\n    tokenizer=fullft_tokenizer,\n    data_collator=data_collator_ff,\n    compute_metrics=compute_metrics,\n    callbacks=[\n        EarlyStoppingCallback(early_stopping_patience=2)\n    ]\n)\n\n# ------------------------------------------------------------\n# Train + measure training time + compute GPU cost\n# ------------------------------------------------------------\nstart_train_ff = time.time()\ntrainer_full.train()\nend_train_ff = time.time()\n\ntrain_seconds_ff = end_train_ff - start_train_ff\ntrain_hours_ff   = train_seconds_ff / 3600\n\nprint(f\"\\n[Full FT] Training time: {train_seconds_ff:.2f} sec ({train_hours_ff:.4f} hours)\")\n\nT4_PRICE = 0.35\ngpu_cost_ff = train_hours_ff * T4_PRICE\nprint(f\"[Full FT] Estimated GPU cost: ${gpu_cost_ff:.4f}\")\n\n# Save model\ntrainer_full.save_model(FULLFT_DIR)\nfullft_tokenizer.save_pretrained(FULLFT_DIR)\n\nprint(\"\\nSaved FULL Fine-Tuned model →\", FULLFT_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T18:59:29.027938Z","iopub.execute_input":"2025-11-22T18:59:29.028516Z","iopub.status.idle":"2025-11-22T19:10:52.825368Z","shell.execute_reply.started":"2025-11-22T18:59:29.028497Z","shell.execute_reply":"2025-11-22T19:10:52.824669Z"}},"outputs":[{"name":"stdout","text":"\n===== SECTION 2B — Full Fine-Tuning DistilRoBERTa =====\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21497 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2c2891e4f3445aaa89bc26bfb4d6229"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2687 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6dfe9ccb692487e9195584511b1bec8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21497 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84c5ee21e3f54fe4a1d2faf6ecf1c52a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2687 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3622dd36176e47658b87346574737950"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10752' max='10752' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10752/10752 11:20, Epoch 8/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.004800</td>\n      <td>0.023428</td>\n      <td>0.995906</td>\n      <td>0.995925</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.000900</td>\n      <td>0.022216</td>\n      <td>0.997395</td>\n      <td>0.997398</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.000300</td>\n      <td>0.021079</td>\n      <td>0.997023</td>\n      <td>0.997026</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.000200</td>\n      <td>0.026505</td>\n      <td>0.997395</td>\n      <td>0.997400</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.000100</td>\n      <td>0.020718</td>\n      <td>0.997767</td>\n      <td>0.997768</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.000100</td>\n      <td>0.014847</td>\n      <td>0.998139</td>\n      <td>0.998139</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.000000</td>\n      <td>0.020238</td>\n      <td>0.997767</td>\n      <td>0.997768</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.000200</td>\n      <td>0.019966</td>\n      <td>0.997767</td>\n      <td>0.997768</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\n[Full FT] Training time: 680.53 sec (0.1890 hours)\n[Full FT] Estimated GPU cost: $0.0662\n\nSaved FULL Fine-Tuned model → /kaggle/working/distilroberta_full_ft\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# **SECTION 3 — DistilRoBERTa-LoRA Inference Pipeline**\n#### *Load Model • Prepare Validation Set • Build Classifier • Run Inference • Evaluate Performance*\n---","metadata":{"_uuid":"7c06541d-03ce-4484-9859-a4d20d0ef524","_cell_guid":"445e9c0e-20cf-4b93-810a-5ecc8ee44469","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ============================================================\n# STEP 0 — Load DistilRoBERTa-LoRA model\n# ============================================================\n\n\n# Path or HF repo name for your DistilRoBERTa-LoRA model\nMODEL_DIR = ROOT + \"/working/distilroberta_lora_ft\"\n\n# ------------------------------------------------------------\n# 1) Load LoRA config to know which base model to use\n# ------------------------------------------------------------\npeft_config = PeftConfig.from_pretrained(MODEL_DIR)\nbase_model_name = peft_config.base_model_name_or_path  # e.g. \"distilroberta-base\"\n\n# ------------------------------------------------------------\n# 2) Define your label mapping (must match training!)\n# ------------------------------------------------------------\nlabels = sorted(train_df[\"intent\"].unique())\nlabel2id = {lbl: i for i, lbl in enumerate(labels)}\nid2label = {i: lbl for lbl, i in label2id.items()}\nnum_labels = len(labels)  # should be 27\n\n# ------------------------------------------------------------\n# 3) Load tokenizer from the *base* model\n# ------------------------------------------------------------\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n\n# ------------------------------------------------------------\n# 4) Load base DistilRoBERTa model with correct num_labels\n# ------------------------------------------------------------\nbase_model = AutoModelForSequenceClassification.from_pretrained(\n    base_model_name,\n    num_labels=num_labels,\n    id2label=id2label,\n    label2id=label2id,\n)\n\n# ------------------------------------------------------------\n# 5) Load LoRA adapter weights on top of the base model\n# ------------------------------------------------------------\nmodel = PeftModel.from_pretrained(base_model, MODEL_DIR)\n\n# Device setup (use GPU if available)\nlora_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using:\", lora_device)\n\nmodel.to(lora_device)\nmodel.eval()\n\n\n# ============================================================\n# STEP 1 — Load validation set & sample 270 items\n# (10 samples per intent as required by assignment)\n# ============================================================\n\nval_df = pd.read_csv(ROOT + \"working/val.csv\")\n\nsample_df = (\n    val_df.groupby(\"intent\")\n          .head(10)\n          .reset_index(drop=True)\n)\n\nprint(\"Sample size:\", sample_df.shape)\nsample_df.head()\n\n\n# ============================================================\n# STEP 2 — Prepare intent list for LLM prompt\n# ============================================================\n\nall_intents = sorted(val_df[\"intent\"].unique())\nintent_list_str = \", \".join(all_intents)\nprint(\"Loaded intents:\", len(all_intents))\n\n# Bullet-style list\navailable_intents_bulleted = \"\\n- \" + \"\\n- \".join(all_intents)\n\n\n# ============================================================\n# STEP 3 — Define DistilRoBERTa-LoRA classifier\n# ============================================================\n\ndef _id_to_label(idx: int) -> str:\n    \"\"\"\n    Map prediction index back to the string label using model.config.id2label.\n    Works whether id2label is a dict or a list.\n    \"\"\"\n    id2label = model.config.id2label\n    if isinstance(id2label, dict):\n        # keys may be \"0\", \"1\", ... or ints\n        return id2label.get(str(idx), id2label.get(idx, str(idx)))\n    return id2label[idx]\n\n\ndef classify_distilroberta_lora(instruction: str) -> str:\n    \"\"\"\n    Classify a single instruction using the DistilRoBERTa-LoRA model.\n    \"\"\"\n    encoded = tokenizer(\n        instruction,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=128,\n        return_tensors=\"pt\",\n    )\n\n    # Send tensors to the same device as the model\n    encoded = {k: v.to(lora_device) for k, v in encoded.items()}\n\n    with torch.no_grad():\n        outputs = model(**encoded)\n        logits = outputs.logits\n        pred_id = int(torch.argmax(logits, dim=-1))\n\n    return _id_to_label(pred_id)\n\n\n# ============================================================\n# STEP 4 — Run inference on all 270 samples using DistilRoBERTa-LoRA\n# ============================================================\n\nval_preds = []\nstart = time.time()\n\n# Run inference on validation samples\nfor text in sample_df[\"instruction\"]:\n    val_preds.append(classify_distilroberta_lora(text))\n\nend = time.time()\nelapsed = end - start\n\n# Compute metrics\nacc = accuracy_score(sample_df[\"intent\"], val_preds)\nf1 = f1_score(sample_df[\"intent\"], val_preds, average=\"macro\")\n\n# Timing breakdown\ntime_per_sample = elapsed / len(sample_df)\ntime_per_1000 = time_per_sample * 1000\n\nprint(\"\\n===== VALIDATION RESULTS =====\")\nprint(\"Accuracy:\", round(acc, 4))\nprint(\"Macro-F1:\", round(f1, 4))\n\nprint(\"\\n===== INFERENCE TIME =====\")\nprint(\"Total time:\", round(elapsed, 2), \"seconds\")\nprint(\"Time per sample:\", round(time_per_sample, 4), \"seconds\")\nprint(\"Time per 1000 samples:\", round(time_per_1000, 2), \"seconds\")","metadata":{"_uuid":"3a28dc90-24ec-46a6-90a4-b60cf025671f","_cell_guid":"f657c889-e1ba-488c-8312-5b384d61ecc4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-22T19:10:52.826125Z","iopub.execute_input":"2025-11-22T19:10:52.826361Z","iopub.status.idle":"2025-11-22T19:10:55.965274Z","shell.execute_reply.started":"2025-11-22T19:10:52.826343Z","shell.execute_reply":"2025-11-22T19:10:55.964464Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Using: cuda\nSample size: (270, 2)\nLoaded intents: 27\n\n===== VALIDATION RESULTS =====\nAccuracy: 0.9926\nMacro-F1: 0.9926\n\n===== INFERENCE TIME =====\nTotal time: 1.82 seconds\nTime per sample: 0.0067 seconds\nTime per 1000 samples: 6.73 seconds\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# **SECTION 3B — DistilRoBERTa Full FT Inference Pipeline**\n#### *Load Model • Prepare Validation Set • Run Classifier • Evaluate Accuracy & Macro-F1*\n---\n","metadata":{}},{"cell_type":"code","source":"print(\"\\n===== SECTION 3B — Full FT Validation Pipeline =====\")\n\n# Load trained model\nfullft_model = AutoModelForSequenceClassification.from_pretrained(\n    FULLFT_DIR,\n    id2label=id2label,\n    label2id=label2id\n)\n\nfullft_model.to(lora_device)\nfullft_model.eval()\n\ndef classify_fullft(text):\n    enc = fullft_tokenizer(\n        text,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=128,\n        return_tensors=\"pt\"\n    )\n    enc = {k: v.to(lora_device) for k, v in enc.items()}\n    with torch.no_grad():\n        logits = fullft_model(**enc).logits\n        pred_id = int(torch.argmax(logits, -1))\n    return id2label[pred_id]\n\n# Run validation on 270 samples\nval_preds_ff = []\nstart = time.time()\n\nfor text in sample_df[\"instruction\"]:\n    val_preds_ff.append(classify_fullft(text))\n\nend = time.time()\nelapsed_ff = end - start\n\nacc_ff = accuracy_score(sample_df[\"intent\"], val_preds_ff)\nf1_ff  = f1_score(sample_df[\"intent\"], val_preds_ff, average=\"macro\")\n\nprint(\"\\n===== FULL FT VALIDATION RESULTS =====\")\nprint(\"Accuracy:\", round(acc_ff, 4))\nprint(\"Macro-F1:\", round(f1_ff, 4))\nprint(\"Total inference time:\", round(elapsed_ff, 2), \"sec\")\nprint(\"Per 1000 samples:\", round(elapsed_ff / 270 * 1000, 2), \"sec\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T19:10:55.967323Z","iopub.execute_input":"2025-11-22T19:10:55.967552Z","iopub.status.idle":"2025-11-22T19:10:57.638672Z","shell.execute_reply.started":"2025-11-22T19:10:55.967534Z","shell.execute_reply":"2025-11-22T19:10:57.638007Z"}},"outputs":[{"name":"stdout","text":"\n===== SECTION 3B — Full FT Validation Pipeline =====\n\n===== FULL FT VALIDATION RESULTS =====\nAccuracy: 0.9963\nMacro-F1: 0.9963\nTotal inference time: 1.39 sec\nPer 1000 samples: 5.14 sec\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# **SECTION 4 — Test Set Preprocessing & Translation Pipeline**\n#### *Load raw test.csv • Apply Cleaning Pipeline • Placeholder Filling • Translation • Category Tagging*\n---","metadata":{"_uuid":"c1adf1a0-d59d-4e47-a138-d6a0161b57cd","_cell_guid":"d47d3e1c-63dd-4e1b-a0a9-81f39a450bb3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ============================================================\n# STEP A — Load raw test.csv\n# ============================================================\n\nTEST_PATH = ROOT + \"input/test-csv/test.csv\"\ntest_raw = pd.read_csv(TEST_PATH)\n\nprint(\"Loaded raw test:\", test_raw.shape)\ntest_raw.head()\n\n\n# ============================================================\n# STEP B — Apply EXACT SAME CLEANING PIPELINE as TRAIN\n# ============================================================\n\n# --- 1) Fix mojibake ---\ntest_raw[\"instruction\"] = (\n    test_raw[\"instruction\"]\n        .astype(str)\n        .apply(fix_mojibake)\n)\n\n# --- 2) Replace placeholders ---\ntest_raw = fill_instruction_placeholders(\n    test_raw,\n    col=\"instruction\",\n    base_seed=123,      # same as training\n    locale=\"en_US\",     # same as training\n)\n\n# --- 3) Clean garbage ---\ntest_raw[\"instruction\"] = test_raw[\"instruction\"].apply(clean_garbage)\n\n# --- 4) Translation (Helsinki mul → en) ---\n\ntranslation_model_name = \"Helsinki-NLP/opus-mt-mul-en\"\nmt_tokenizer = MarianTokenizer.from_pretrained(translation_model_name)\n\nmt_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Translation model using:\", mt_device)\n\nmt_model = MarianMTModel.from_pretrained(translation_model_name).to(mt_device)\n\ndef translate_to_en(text: str) -> str:\n    batch = mt_tokenizer([text], return_tensors=\"pt\", truncation=True)\n    batch = {k: v.to(mt_device) for k, v in batch.items()}  # SAME DEVICE\n    gen = mt_model.generate(**batch, max_new_tokens=128)\n    return mt_tokenizer.decode(gen[0], skip_special_tokens=True)\n\ntest_raw[\"instruction\"] = test_raw[\"instruction\"].apply(translate_to_en)\n\n\n# --- 5) Add category tag (if missing, mark as UNKNOWN) ---\nif \"category\" in test_raw.columns:\n    test_raw[\"instruction\"] = test_raw.apply(\n        lambda row: f\"[CATEGORY={row['category']}] \" + row[\"instruction\"],\n        axis=1,\n    )\nelse:\n    def add_tag_auto(t):\n        m = re.search(r\"\\[CATEGORY=([A-Z_]+)\\]\", t)\n        if m:\n            return t\n        return \"[CATEGORY=UNKNOWN] \" + t\n\n    test_raw[\"instruction\"] = test_raw[\"instruction\"].apply(add_tag_auto)\n\n\nprint(\"\\n=== Cleaned test sample ===\")\nprint(test_raw.head())","metadata":{"_uuid":"0f5c2ec2-e38b-4d1d-9584-44d865c7c817","_cell_guid":"00ea2689-ae50-4e1f-a67f-f3b7fc4a8b17","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-22T19:10:57.639350Z","iopub.execute_input":"2025-11-22T19:10:57.639570Z","iopub.status.idle":"2025-11-22T19:12:19.643002Z","shell.execute_reply.started":"2025-11-22T19:10:57.639543Z","shell.execute_reply":"2025-11-22T19:12:19.642095Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Loaded raw test: (270, 5)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc079f172b6347b7b5eb9e012a57c7a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/707k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c38e963a400f4eb39ec71d3f84f0a928"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/791k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8e4f314f9ed4012b038475d52921737"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f6cb4543aa84369b057d67f9492bb8e"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97d3186632be4fca8ec09d0ad26fe086"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n","output_type":"stream"},{"name":"stdout","text":"Translation model using: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/310M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e639182a3d8644acba1a43d41c8f798a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e35fc050d144dddbd1c4c9add98a852"}},"metadata":{}},{"name":"stdout","text":"\n=== Cleaned test sample ===\n   id flags                                        instruction category  \\\n0   1    BQ  [CATEGORY=CONTACT] Could you please let me kno...  CONTACT   \n1   2    BQ  [CATEGORY=ACCOUNT] I'm trying to switch from m...  ACCOUNT   \n2   3     B  [CATEGORY=CONTACT] Can someone from customer s...  CONTACT   \n3   4    BQ  [CATEGORY=ACCOUNT] Could you guide me through ...  ACCOUNT   \n4   5    BQ  [CATEGORY=CONTACT] I've tried to solve my prob...  CONTACT   \n\n                                            response  \n0  Thank you for contacting us. We will assist yo...  \n1  Thank you for contacting us. We will assist yo...  \n2  Thank you for contacting us. We will assist yo...  \n3  Thank you for contacting us. We will assist yo...  \n4  Thank you for contacting us. We will assist yo...  \n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# **SECTION 5 — DistilRoBERTa-LoRA Test Set Inference & Submission**\n#### *Load Classifier • Run Test Inference • Build Submission • Compute Inference Cost*\n---","metadata":{"_uuid":"2b26f80b-a11a-443f-bbda-455b0bc02f9f","_cell_guid":"a45ccce3-a22a-44fb-8e19-2982a4471230","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ============================================================\n# STEP C0 — Load DistilRoBERTa-LoRA classifier for inference\n# ============================================================\n\nMODEL_DIR = ROOT + \"/working/distilroberta_lora_ft\"  # your LoRA folder\n\n# ------------------------------------------------------------\n# 1) Read LoRA config to get base model name\n# ------------------------------------------------------------\npeft_config = PeftConfig.from_pretrained(MODEL_DIR)\nbase_model_name = peft_config.base_model_name_or_path  # e.g. \"distilroberta-base\"\nprint(\"LoRA base model:\", base_model_name)\n\n# ------------------------------------------------------------\n# 2) Recreate label mapping (must match training)\n# ------------------------------------------------------------\nINTENT_LABELS = [\n    \"cancel_order\",\n    \"change_order\",\n    \"change_shipping_address\",\n    \"check_cancellation_fee\",\n    \"check_invoice\",\n    \"check_payment_methods\",\n    \"check_refund_policy\",\n    \"complaint\",\n    \"contact_customer_service\",\n    \"contact_human_agent\",\n    \"create_account\",\n    \"delete_account\",\n    \"delivery_options\",\n    \"delivery_period\",\n    \"edit_account\",\n    \"get_invoice\",\n    \"get_refund\",\n    \"newsletter_subscription\",\n    \"payment_issue\",\n    \"place_order\",\n    \"recover_password\",\n    \"registration_problems\",\n    \"review\",\n    \"set_up_shipping_address\",\n    \"switch_account\",\n    \"track_order\",\n    \"track_refund\",\n]\n\nlabel2id = {lbl: i for i, lbl in enumerate(INTENT_LABELS)}\nid2label = {i: lbl for lbl, i in label2id.items()}\n\n# ------------------------------------------------------------\n# 3) Load tokenizer for the classifier\n# ------------------------------------------------------------\nclf_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n\n# ------------------------------------------------------------\n# 4) Load base DistilRoBERTa with correct num_labels\n# ------------------------------------------------------------\nbase_clf = AutoModelForSequenceClassification.from_pretrained(\n    base_model_name,\n    num_labels=len(INTENT_LABELS),\n    id2label=id2label,\n    label2id=label2id,\n)\n\n# ------------------------------------------------------------\n# 5) Load LoRA adapter weights\n# ------------------------------------------------------------\nclf_model = PeftModel.from_pretrained(base_clf, MODEL_DIR)\n\nclf_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nclf_model.to(clf_device)\nclf_model.eval()\n\n# ------------------------------------------------------------\n# C0.5 — Report Model Size (Base + LoRA Trainable Parameters)\n# ------------------------------------------------------------\ndef count_parameters(model):\n    total = sum(p.numel() for p in model.parameters())\n    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    return total, trainable\n\nbase_total, base_trainable = count_parameters(base_clf)\nprint(f\"\\nBase Model Total Params: {base_total:,}\")\nprint(f\"Base Model Trainable Params (all frozen): {base_trainable:,}\")\n\nlora_total, lora_trainable = count_parameters(clf_model)\nprint(f\"\\nLoRA+Base Total Params: {lora_total:,}\")\nprint(f\"LoRA Trainable Params (adapter only): {lora_trainable:,}\")\n\nprint(\"\\n=== Model Size Report Completed ===\\n\")\n\n\n\n# ------------------------------------------------------------\n# Classification function\n# ------------------------------------------------------------\ndef classify_distilroberta_lora(text: str) -> str:\n    enc = clf_tokenizer(\n        text,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=128,\n        return_tensors=\"pt\",\n    )\n    enc = {k: v.to(clf_device) for k, v in enc.items()}\n\n    with torch.no_grad():\n        outputs = clf_model(**enc)\n        logits = outputs.logits\n        pred_id = int(torch.argmax(logits, dim=-1))\n\n    return id2label[pred_id]\n\nprint(\"LoRA base model:\", base_model_name)\n\n\n# ============================================================\n# STEP C — Run inference on test rows (DistilRoBERTa-LoRA)\n# ============================================================\n\n\ntest_preds = []\nstart = time.time()\n\nfor i, row in test_raw.iterrows():\n    msg = row[\"instruction\"]\n    pred = classify_distilroberta_lora(msg)\n    test_preds.append(pred)\n\n    if i % 20 == 0:\n        print(f\"Processed {i}/{len(test_raw)}\")\n\nend = time.time()\nelapsed_test = end - start\n\n\n# ============================================================\n# STEP D — Build submission file\n# ============================================================\n\nsubmission = pd.DataFrame({\n    \"id\": test_raw[\"id\"],\n    \"intent\": test_preds\n})\n\nsub_path = ROOT + \"working/submission.csv\"\nsubmission.to_csv(sub_path, index=False)\n\nprint(\"\\nSaved submission →\", sub_path)\nprint(submission.head())\n\n\n# ============================================================\n# STEP E — Report inference speed + cost\n# ============================================================\n\ntime_per_sample_test = elapsed_test / len(test_raw)\ntime_per_1000_test = time_per_sample_test * 1000\n\nprint(\"\\n===== INFERENCE REPORT (TEST SET) =====\")\nprint(\"Total inference time:\", round(elapsed_test, 2), \"seconds\")\nprint(\"Estimated time per 1000 samples:\", round(time_per_1000_test, 2), \"seconds\")\n\n# GPU Cost Calculation (T4)\nT4_PRICE_PER_HOUR = 0.35\ninfer_hours_per_1000 = time_per_1000_test / 3600\ngpu_infer_cost_per_1000 = infer_hours_per_1000 * T4_PRICE_PER_HOUR\n\nprint(f\"Estimated Inference GPU Cost per 1000 samples (T4): ${gpu_infer_cost_per_1000:.6f}\")\nprint(\"===== DONE =====\")","metadata":{"_uuid":"12351a2f-ef36-428d-a2f6-56103829554b","_cell_guid":"f16d9dc2-ae20-4136-9c66-d028e403aeae","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-22T19:12:19.643956Z","iopub.execute_input":"2025-11-22T19:12:19.644265Z","iopub.status.idle":"2025-11-22T19:12:22.313420Z","shell.execute_reply.started":"2025-11-22T19:12:19.644221Z","shell.execute_reply":"2025-11-22T19:12:22.312733Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"LoRA base model: distilroberta-base\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nBase Model Total Params: 83,340,342\nBase Model Trainable Params (all frozen): 611,355\n\nLoRA+Base Total Params: 83,340,342\nLoRA Trainable Params (adapter only): 611,355\n\n=== Model Size Report Completed ===\n\nLoRA base model: distilroberta-base\nProcessed 0/270\nProcessed 20/270\nProcessed 40/270\nProcessed 60/270\nProcessed 80/270\nProcessed 100/270\nProcessed 120/270\nProcessed 140/270\nProcessed 160/270\nProcessed 180/270\nProcessed 200/270\nProcessed 220/270\nProcessed 240/270\nProcessed 260/270\n\nSaved submission → /kaggle/working/submission.csv\n   id                    intent\n0   1  contact_customer_service\n1   2            switch_account\n2   3  contact_customer_service\n3   4            create_account\n4   5       contact_human_agent\n\n===== INFERENCE REPORT (TEST SET) =====\nTotal inference time: 1.86 seconds\nEstimated time per 1000 samples: 6.89 seconds\nEstimated Inference GPU Cost per 1000 samples (T4): $0.000670\n===== DONE =====\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# **SECTION 5B — DistilRoBERTa Full FT Test Set Inference & Submission**\n#### *Load Full-FT Model • Predict Test Rows • Build Submission • Compute Inference Cost*\n---\n","metadata":{}},{"cell_type":"code","source":"print(\"\\n===== SECTION 5B — Full FT Test Inference & Submission =====\")\n\ntest_preds_ff = []\nstart = time.time()\n\nfor i, row in test_raw.iterrows():\n    pred = classify_fullft(row[\"instruction\"])\n    test_preds_ff.append(pred)\n\nend = time.time()\nelapsed_test_ff = end - start\n\nsubmission_ff = pd.DataFrame({\n    \"id\": test_raw[\"id\"],\n    \"intent\": test_preds_ff\n})\n\nsub_path_ff = ROOT + \"working/submission_fullft.csv\"\nsubmission_ff.to_csv(sub_path_ff, index=False)\n\nprint(\"\\nSaved FULL FT submission:\", sub_path_ff)\n\n# Compute inference cost\ntime_per_sample_ff = elapsed_test_ff / len(test_raw)\ntime_per_1000_ff   = time_per_sample_ff * 1000\n\ninfer_hours_ff = time_per_1000_ff / 3600\ngpu_infer_cost_ff = infer_hours_ff * T4_PRICE\n\nprint(\"\\n===== FULL FT TEST INFERENCE REPORT =====\")\nprint(\"Inference time:\", round(elapsed_test_ff, 2), \"sec\")\nprint(\"Per 1000 samples:\", round(time_per_1000_ff, 2), \"sec\")\nprint(f\"Estimated GPU inference cost (per 1000): ${gpu_infer_cost_ff:.6f}\")\nprint(\"===== DONE (FULL FINE-TUNING) =====\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T19:12:22.314198Z","iopub.execute_input":"2025-11-22T19:12:22.314455Z","iopub.status.idle":"2025-11-22T19:12:23.784584Z","shell.execute_reply.started":"2025-11-22T19:12:22.314431Z","shell.execute_reply":"2025-11-22T19:12:23.783891Z"}},"outputs":[{"name":"stdout","text":"\n===== SECTION 5B — Full FT Test Inference & Submission =====\n\nSaved FULL FT submission: /kaggle/working/submission_fullft.csv\n\n===== FULL FT TEST INFERENCE REPORT =====\nInference time: 1.46 sec\nPer 1000 samples: 5.41 sec\nEstimated GPU inference cost (per 1000): $0.000526\n===== DONE (FULL FINE-TUNING) =====\n","output_type":"stream"}],"execution_count":10}]}
